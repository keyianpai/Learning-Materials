{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.2\n",
      "2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "text = open(\"nietzsche.txt\").read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "maxlen = 60\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "WARNING:tensorflow:From /home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 110s 549us/step - loss: 1.9801\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 107s 535us/step - loss: 1.6356\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 539us/step - loss: 1.5437\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 540us/step - loss: 1.4993\n",
      "epoch 5\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 538us/step - loss: 1.4702\n",
      "epoch 6\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 538us/step - loss: 1.4489\n",
      "epoch 7\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 539us/step - loss: 1.4318\n",
      "epoch 8\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 540us/step - loss: 1.4195\n",
      "epoch 9\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 539us/step - loss: 1.4077\n",
      "epoch 10\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 109s 542us/step - loss: 1.3963\n",
      "epoch 11\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 538us/step - loss: 1.3914\n",
      "epoch 12\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 538us/step - loss: 1.3848\n",
      "epoch 13\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 539us/step - loss: 1.3782\n",
      "epoch 14\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 538us/step - loss: 1.3728\n",
      "epoch 15\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 540us/step - loss: 1.3668\n",
      "epoch 16\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 540us/step - loss: 1.3621\n",
      "epoch 17\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 542us/step - loss: 1.3568\n",
      "epoch 18\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 540us/step - loss: 1.3547\n",
      "epoch 19\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 538us/step - loss: 1.3503\n",
      "epoch 20\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 539us/step - loss: 1.3481\n",
      "epoch 21\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 111s 554us/step - loss: 1.3456\n",
      "epoch 22\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 115s 575us/step - loss: 1.3421\n",
      "epoch 23\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 115s 575us/step - loss: 1.3434\n",
      "epoch 24\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 114s 568us/step - loss: 1.3380\n",
      "epoch 25\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 114s 570us/step - loss: 1.3326\n",
      "epoch 26\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 542us/step - loss: 1.3316\n",
      "epoch 27\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 106s 531us/step - loss: 1.3305\n",
      "epoch 28\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 106s 529us/step - loss: 1.3327\n",
      "epoch 29\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 106s 528us/step - loss: 1.3277\n",
      "epoch 30\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 106s 529us/step - loss: 1.3281\n",
      "epoch 31\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 105s 523us/step - loss: 1.3309\n",
      "epoch 32\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 104s 519us/step - loss: 1.3268\n",
      "epoch 33\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 103s 515us/step - loss: 1.3250\n",
      "epoch 34\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 104s 517us/step - loss: 1.3296\n",
      "epoch 35\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 103s 517us/step - loss: 1.3361\n",
      "epoch 36\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 104s 518us/step - loss: 1.3260\n",
      "epoch 37\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 103s 516us/step - loss: 1.3273\n",
      "epoch 38\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 103s 515us/step - loss: 1.3239\n",
      "epoch 39\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 104s 518us/step - loss: 1.3250\n",
      "epoch 40\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 103s 517us/step - loss: 1.3245\n",
      "epoch 41\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 104s 519us/step - loss: 1.3275\n",
      "epoch 42\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 104s 518us/step - loss: 1.3323\n",
      "epoch 43\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 103s 515us/step - loss: 1.3322\n",
      "epoch 44\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 103s 515us/step - loss: 1.3278\n",
      "epoch 45\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 104s 517us/step - loss: 1.3266\n",
      "epoch 46\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 104s 518us/step - loss: 1.3258\n",
      "epoch 47\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 107s 536us/step - loss: 1.3248\n",
      "epoch 48\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 539us/step - loss: 1.3234\n",
      "epoch 49\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 107s 534us/step - loss: 1.3318\n",
      "epoch 50\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 538us/step - loss: 1.3269\n",
      "epoch 51\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 106s 531us/step - loss: 1.3260\n",
      "epoch 52\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 540us/step - loss: 1.3361\n",
      "epoch 53\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 107s 535us/step - loss: 1.3284\n",
      "epoch 54\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 538us/step - loss: 1.3450\n",
      "epoch 55\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 107s 534us/step - loss: 1.3407\n",
      "epoch 56\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 107s 535us/step - loss: 1.3849\n",
      "epoch 57\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 540us/step - loss: 1.3216\n",
      "epoch 58\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 107s 535us/step - loss: 1.3246\n",
      "epoch 59\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 108s 542us/step - loss: 1.3516\n",
      "--- Generating with seed: \"fing and depersonalizing of the spirit has recently been\n",
      "cel\"\n",
      "------ temperature: 0.2\n",
      "fing and depersonalizing of the spirit has recently been\n",
      "celemons to the metaphysical the strength of the interpretation of the same the comparison of the moral protection of the strength of the present and the command of the protection of the same the protection of the same things for the protection of the things to be an attack to the strength of the world of the strength and in the same the protection of the proper the same the strength of the present a\n",
      "------ temperature: 0.5\n",
      "tection of the proper the same the strength of the present and demonstances that cause of the matter who has been sense and comparison--reverent with the domanible,\n",
      "can not the in the fact that he is only and its fact that the fathers of the most powerful the courtest\n",
      "of the order of the false the good and the concerns of thoughts soul is always and virtue protection of disciers, which has or one have to be the god\"; in the strang\" of the lof and and dispo\n",
      "------ temperature: 1.0\n",
      "have to be the god\"; in the strang\" of the lof and and dispotenes serve\n",
      "decumes case over sacrifice long, and to imilation.\n",
      "\n",
      "\n",
      "the place"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ideas then \"even good fypenas, of platey is correspening in its midition.\n",
      "\n",
      "unrount of history is the ethisg\n",
      "bead. a\n",
      "dlent character wastist of\n",
      "which the\n",
      "scholars by the\n",
      "hardved the the\n",
      "a literats under the ethic.=--and bake when the marbirmeno-unbestigator of \"command\n",
      "fold followered the last was \"should\n",
      "purpoing\"--it, for\n",
      "------ temperature: 1.2\n",
      "mand\n",
      "fold followered the last was \"should\n",
      "purpoing\"--it, for inagse-ity say us or not thinking and dant, ealst are savager it free\n",
      "pateoution\n",
      "of could of one's quigency, himself period cruelty; had to-tloodemands\n",
      "abour, such a conscience, ovinal exchistience vouonflous cured,\n",
      "every\n",
      "one slave-curcursmentarly tradern with german darists artists. one about my owingness, understand evil mivudedened\n",
      "man\n",
      "what the ofny will hught by has\n",
      "breadible possible, under \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 60):\n",
    "    print('epoch', epoch)\n",
    "    model.fit(X, y, batch_size=128, epochs=1)\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated_text = text[start_index: start_index + maxlen]\n",
    "print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(generated_text)\n",
    "    # Generate 400 characters\n",
    "    for i in range(400):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qinhanmin-test",
   "language": "python",
   "name": "qinhanmin-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
